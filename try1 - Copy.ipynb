{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Read the Excel file into a pandas DataFrame\n",
    "df = pd.read_excel('dataset.xlsx')\n",
    "df['KOSTENRUBRIEK - declared'] = df['KOSTENRUBRIEK - declared'].str.lower()\n",
    "df['FLC: REDEN VERWERPING'] = df['FLC: REDEN VERWERPING'].str.lower()\n",
    "df['BESCHRIJVING DECLARATIE'] = df['BESCHRIJVING DECLARATIE'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unique flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Flags:\n",
      "the cost was declared in the wrong category\n",
      "double declaration\n",
      "please specify the correct invoiced amount\n",
      "costs that are declared too late, can only be reimbursed for 50%\n",
      "purchase not eligible for subsidy\n",
      "please specify the correct supplier\n"
     ]
    }
   ],
   "source": [
    "# Extract unique flags from the \"FLC: REDEN VERWERPING\" column\n",
    "all_flags = df[\"FLC: REDEN VERWERPING\"].str.split('|', expand=True).stack().str.strip().unique()\n",
    "\n",
    "# Print the unique flags\n",
    "print(\"Unique Flags:\")\n",
    "for flag in all_flags:\n",
    "    print(flag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the phrases to check for in the specified column\n",
    "phrases = {\n",
    "    \"FLC: REDEN VERWERPING\": {\n",
    "        \"double declaration\": \"double\",\n",
    "        \"too late\": \"late\",\n",
    "        \"correct invoiced amount\": \"amount\",\n",
    "        \"supplier\": \"supplier\",\n",
    "        \"wrong category\": \"category\",\n",
    "        \"eligible\": \"eligible\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Iterate through the phrases and update the corresponding columns\n",
    "for column_name, conditions in phrases.items():\n",
    "    for phrase, new_column in conditions.items():\n",
    "        df[new_column] = df[column_name].str.contains(phrase, case=False, na=False).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred.double'] = 0\n",
    "\n",
    "# Check if an entry has the same values in both columns 'REFERENTIE FACTUUR' and 'DATUM FACTUUR - DECLARED'\n",
    "duplicate_entries = df[df.duplicated(['REFERENTIE FACTUUR', 'DATUM FACTUUR - DECLARED'], keep=False)]\n",
    "\n",
    "for idx, group in duplicate_entries.groupby(['REFERENTIE FACTUUR', 'DATUM FACTUUR - DECLARED']):\n",
    "    # Find the earliest value in 'DECLARATIEDATUM (can be assumed to be close to payment date)' for this entry\n",
    "    earliest_date = group['DECLARATIEDATUM (can be assumed to be close to payment date)'].min()\n",
    "    \n",
    "    # Set 'pred.double' to 1 for entries other than the one with the earliest date\n",
    "    df.loc[group.index, 'pred.double'] = (group['DECLARATIEDATUM (can be assumed to be close to payment date)'] != earliest_date).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too late"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the \"DATUM FACTUUR - DECLARED\" column to datetime format\n",
    "df['DATUM FACTUUR - DECLARED'] = pd.to_datetime(df['DATUM FACTUUR - DECLARED'], errors='coerce')\n",
    "\n",
    "# Create the \"latest\" column by moving the month 6 months forward\n",
    "df['latest'] = df['DATUM FACTUUR - DECLARED'] + pd.DateOffset(months=6)\n",
    "\n",
    "# Convert date columns to datetime objects\n",
    "df['latest'] = pd.to_datetime(df['latest'])\n",
    "df['DECLARATIEDATUM (can be assumed to be close to payment date)'] = pd.to_datetime(df['DECLARATIEDATUM (can be assumed to be close to payment date)'])\n",
    "\n",
    "# Calculate the time difference and create a new column 'timediff'\n",
    "df['pred.late'] = (df['DECLARATIEDATUM (can be assumed to be close to payment date)'] - df['latest']).dt.days > 0\n",
    "df['pred.late'] = df['pred.late'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrong amount (only EUR works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pred.amount'] = (df['BETAALD BEDRAG - extracted from invoice'] - df['BETAALD BEDRAG - declared ']).apply(lambda x: 1 if x < 0 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrong supplier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "se = SnowballStemmer(\"english\")\n",
    "\n",
    "x = df[\"LEVERANCIER - EXTRACTED\"]\n",
    "y = df[\"LEVERANCIER - DECLARED\"]\n",
    "\n",
    "def clean_message(message_list):\n",
    "    ms = []\n",
    "    for word in message_list:\n",
    "        ms.append(se.stem(word))\n",
    "    return ms \n",
    "\n",
    "def lever_equals(x, y):\n",
    "    if x == y:\n",
    "        return 0\n",
    "    x = re.sub(r'[^\\w\\s]', ' ', x)\n",
    "    y = re.sub(r'[^\\w\\s]', ' ', y)\n",
    "    split_x = clean_message(word_tokenize(x.lower()))\n",
    "    split_y = clean_message(word_tokenize(y.lower()))\n",
    "    \n",
    "    if len(split_x) == len(split_y):\n",
    "        for i in range(len(split_x)):\n",
    "            if split_x[i][0] != split_y[i][0]:\n",
    "                return 1\n",
    "        return 0\n",
    "    elif len(split_x)!=0 and len(split_y)!=0:\n",
    "        mini = min(len(split_x), len(split_y))\n",
    "        for i in range(mini):\n",
    "            if split_x[i] != split_y[i]:\n",
    "                return 1\n",
    "        return 0\n",
    "    return 1\n",
    "\n",
    "# Create a new column 'pred.supplier' in df and store the results\n",
    "df['pred.supplier'] = [lever_equals(x[i], y[i]) for i in range(len(x))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrong category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.92\n",
      "\n",
      "Classification Report:\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                  equipment       0.95      0.95      0.95        19\n",
      "          external services       0.87      0.91      0.89        22\n",
      "infrastructure and building       0.94      0.94      0.94        16\n",
      "          preparation costs       0.90      0.82      0.86        22\n",
      "               travel costs       0.94      1.00      0.97        16\n",
      "\n",
      "                   accuracy                           0.92        95\n",
      "                  macro avg       0.92      0.92      0.92        95\n",
      "               weighted avg       0.92      0.92      0.92        95\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter rows where \"category\" column has value 0\n",
    "df2 = df[df['category'] == 0].copy()  # Make a copy of the filtered DataFrame\n",
    "\n",
    "# Save the filtered DataFrame to a CSV file\n",
    "df2.to_csv('cat_data.csv', index=False)\n",
    "\n",
    "# Train model on positive targets\n",
    "X = df2['BESCHRIJVING DECLARATIE']\n",
    "y = df2['KOSTENRUBRIEK - declared']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "train_predictions = classifier.predict(X_train_tfidf)\n",
    "\n",
    "# Update 'pred.category' in the original DataFrame for the training set\n",
    "df2.loc[df2['BESCHRIJVING DECLARATIE'].index, 'pred.category'] = (train_predictions != y_train).astype(int)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Update 'pred.category' in the original DataFrame for the test set\n",
    "df2.loc[X_test.index, 'pred.category'] = (test_predictions != y_test).astype(int)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, test_predictions)\n",
    "classification_rep = classification_report(y_test, test_predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('\\nClassification Report:')\n",
    "print(classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (377) does not match length of index (472)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Save the probabilities in separate columns for each category in the original DataFrame for the training set\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, class_label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(classifier\u001b[38;5;241m.\u001b[39mclasses_):\n\u001b[1;32m---> 32\u001b[0m     \u001b[43mdf2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpred.probability_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mclass_label\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m train_probabilities[:, i]\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Make predictions on the test set\u001b[39;00m\n\u001b[0;32m     35\u001b[0m test_predictions \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(X_test_tfidf)\n",
      "File \u001b[1;32mc:\\Users\\camd1\\miniconda3\\envs\\holyhack\\Lib\\site-packages\\pandas\\core\\frame.py:4299\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4296\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   4297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4298\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 4299\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\camd1\\miniconda3\\envs\\holyhack\\Lib\\site-packages\\pandas\\core\\frame.py:4512\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4502\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4503\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4504\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4505\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4510\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4511\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4512\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4515\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4516\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4517\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m   4518\u001b[0m     ):\n\u001b[0;32m   4519\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4520\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\camd1\\miniconda3\\envs\\holyhack\\Lib\\site-packages\\pandas\\core\\frame.py:5253\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   5250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   5252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 5253\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5254\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   5255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5256\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[0;32m   5257\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5260\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[0;32m   5261\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\camd1\\miniconda3\\envs\\holyhack\\Lib\\site-packages\\pandas\\core\\common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (377) does not match length of index (472)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Filter rows where \"category\" column has value 0\n",
    "df2 = df[df['category'] == 0].copy()  # Make a copy of the filtered DataFrame\n",
    "\n",
    "# Train model on positive targets\n",
    "X = df2['BESCHRIJVING DECLARATIE']\n",
    "y = df2['KOSTENRUBRIEK - declared']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data using TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Train a Multinomial Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the training set\n",
    "train_predictions = classifier.predict(X_train_tfidf)\n",
    "train_probabilities = classifier.predict_proba(X_train_tfidf)\n",
    "\n",
    "# Save the probabilities in separate columns for each category in the original DataFrame for the training set\n",
    "for i, class_label in enumerate(classifier.classes_):\n",
    "    df2[f'pred.probability_{class_label}'] = train_probabilities[:, i]\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = classifier.predict(X_test_tfidf)\n",
    "test_probabilities = classifier.predict_proba(X_test_tfidf)\n",
    "\n",
    "# Save the probabilities in separate columns for each category in the original DataFrame for the test set\n",
    "for i, class_label in enumerate(classifier.classes_):\n",
    "    df2[f'pred.probability_{class_label}'] = test_probabilities[:, i]\n",
    "\n",
    "# Update 'pred.category' in the original DataFrame for the training set\n",
    "df2.loc[df2['BESCHRIJVING DECLARATIE'].index, 'pred.category'] = (train_predictions != y_train).astype(int)\n",
    "\n",
    "# Update 'pred.category' in the original DataFrame for the test set\n",
    "df2.loc[X_test.index, 'pred.category'] = (test_predictions != y_test).astype(int)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df2.to_csv('cat_data_with_probabilities.csv', index=False)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, test_predictions)\n",
    "classification_rep = classification_report(y_test, test_predictions)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "print('\\nClassification Report:')\n",
    "print(classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.95\n",
      "\n",
      "Classification Report:\n",
      "                             precision    recall  f1-score   support\n",
      "\n",
      "                  equipment       0.00      0.00      0.00         8\n",
      "          external services       0.00      0.00      0.00         8\n",
      "infrastructure and building       0.00      0.00      0.00         3\n",
      "          preparation costs       0.25      0.50      0.33         2\n",
      "               travel costs       0.00      0.00      0.00         1\n",
      "\n",
      "                   accuracy                           0.05        22\n",
      "                  macro avg       0.05      0.10      0.07        22\n",
      "               weighted avg       0.02      0.05      0.03        22\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use model to predict on positive targets\n",
    "df_category_1 = df[df['category'] == 1].copy()  # Make a copy to avoid modifying the original DataFrame\n",
    "\n",
    "# Extract text data and labels\n",
    "X_category_1 = df_category_1['BESCHRIJVING DECLARATIE']\n",
    "y_category_1 = df_category_1['KOSTENRUBRIEK - declared']\n",
    "\n",
    "# Vectorize the text data using the same TF-IDF vectorizer\n",
    "X_category_1_tfidf = vectorizer.transform(X_category_1)\n",
    "\n",
    "# Make predictions on the instances where 'category' has value 1\n",
    "predictions_category_1 = classifier.predict(X_category_1_tfidf)\n",
    "\n",
    "# Update 'pred.category' in the original DataFrame\n",
    "df_category_1['pred.category'] = 1-(predictions_category_1 == y_category_1).astype(int)\n",
    "\n",
    "# Evaluate the model on these instances\n",
    "accuracy_category_1 = accuracy_score(y_category_1, predictions_category_1)\n",
    "classification_rep_category_1 = classification_report(y_category_1, predictions_category_1)\n",
    "\n",
    "print(f'Accuracy: {1 - accuracy_category_1:.2f}')\n",
    "print('\\nClassification Report:')\n",
    "print(classification_rep_category_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df2, df_category_1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not eligible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 0.98\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the Excel file\n",
    "df_eli = pd.read_excel('eli_data.xlsx')\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_eli['BESCHRIJVING DECLARATIE'], df_eli['eligible'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with TF-IDF vectorizer, stopwords removal, and Decision Tree classifier\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words='english')),  # Using TF-IDF vectorizer with stopwords removal\n",
    "    ('classifier', DecisionTreeClassifier(random_state=42, class_weight=\"balanced\"))\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy on the test set: {accuracy:.2f}\")\n",
    "\n",
    "# Assuming 'df' is your DataFrame for testing\n",
    "# Make predictions on the 'df' DataFrame\n",
    "df['pred.eligible'] = pipeline.predict(df['BESCHRIJVING DECLARATIE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Excel file saved to: output_file.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Save the updated DataFrame to a new Excel file\n",
    "output_file_path = 'output_file.xlsx'\n",
    "df.to_excel(output_file_path, index=False)\n",
    "\n",
    "print(f\"Updated Excel file saved to: {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      double       1.00      0.40      0.57         5\n",
      "        late       1.00      0.98      0.99        50\n",
      "      amount       1.00      0.39      0.56        18\n",
      "    supplier       0.62      0.89      0.73         9\n",
      "    category       0.54      0.95      0.69        22\n",
      "    eligible       1.00      1.00      1.00         6\n",
      "\n",
      "   micro avg       0.80      0.85      0.82       110\n",
      "   macro avg       0.86      0.77      0.76       110\n",
      "weighted avg       0.88      0.85      0.82       110\n",
      " samples avg       0.16      0.16      0.16       110\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2831   23]\n",
      " [  17   93]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\camd1\\miniconda3\\envs\\holyhack\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\camd1\\miniconda3\\envs\\holyhack\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in samples with no true labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\camd1\\miniconda3\\envs\\holyhack\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Extract the actual values\n",
    "y_true = df[['double', 'late', 'amount', 'supplier', 'category', 'eligible']].values\n",
    "\n",
    "# Extract the predicted values\n",
    "y_pred = df[['pred.double', 'pred.late', 'pred.amount', 'pred.supplier', 'pred.category', 'pred.eligible']].values\n",
    "\n",
    "# Calculate classification report\n",
    "classification_rep = classification_report(y_true, y_pred, target_names=['double', 'late', 'amount', 'supplier', 'category', 'eligible'])\n",
    "print(\"Classification Report:\")\n",
    "print(classification_rep)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_matrix = confusion_matrix(y_true.flatten(), y_pred.flatten())\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Rates:\n",
      "double: 0.9939\n",
      "late: 0.9980\n",
      "amount: 0.9777\n",
      "supplier: 0.9879\n",
      "category: 0.9615\n",
      "eligible: 1.0000\n",
      "\n",
      "Mistakes:\n",
      "double: 0.0061\n",
      "late: 0.0020\n",
      "amount: 0.0223\n",
      "supplier: 0.0121\n",
      "category: 0.0385\n",
      "eligible: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Initialize dictionaries to store accuracy rates and proportions of wrong predictions\n",
    "accuracy_rates = {}\n",
    "proportions_of_wrongs = {}\n",
    "\n",
    "# Iterate over each feature\n",
    "for feature in ['double', 'late', 'amount', 'supplier', 'category', 'eligible']:\n",
    "    # Extract the actual values for the current feature\n",
    "    y_true_feature = df[feature].values\n",
    "    \n",
    "    # Extract the predicted values for the current feature\n",
    "    y_pred_feature = df[f'pred.{feature}'].values\n",
    "    \n",
    "    # Calculate accuracy rate for the current feature\n",
    "    accuracy = accuracy_score(y_true_feature, y_pred_feature)\n",
    "    \n",
    "    # Store accuracy rate in the dictionary\n",
    "    accuracy_rates[feature] = accuracy\n",
    "    \n",
    "    # Calculate proportion of wrong predictions for the current feature\n",
    "    wrong_predictions = (y_true_feature != y_pred_feature).sum()\n",
    "    total_predictions = len(y_true_feature)\n",
    "    proportion_of_wrongs = wrong_predictions / total_predictions\n",
    "    \n",
    "    # Store mistakes predictions in the dictionary\n",
    "    proportions_of_wrongs[feature] = proportion_of_wrongs\n",
    "\n",
    "# Print accuracy rates\n",
    "print(\"Accuracy Rates:\")\n",
    "for feature, accuracy in accuracy_rates.items():\n",
    "    print(f\"{feature}: {accuracy:.4f}\")\n",
    "\n",
    "# Print mistakes\n",
    "print(\"\\nMistakes:\")\n",
    "for feature, proportion_of_wrongs in proportions_of_wrongs.items():\n",
    "    print(f\"{feature}: {proportion_of_wrongs:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "holyhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
